We need to design the **FactSales** table in **Azure Synapse Analytics dedicated SQL pool** with the following requirements:

* Data will be retained for **five years**.
* Each year, data older than five years must be deleted.
* Data must be **distributed evenly across distributions**.
* Must **minimize deletion time** â†’ use **partitioning** by `OrderDateKey`.

---

### Breakdown of choices:

* **Distribution**:

  * `ROUND_ROBIN` â†’ spreads rows evenly across distributions (âœ… balances load, avoids skew).
  * `HASH` â†’ requires a good distribution column (not specified as necessary here).
  * `REPLICATE` â†’ duplicates data on all distributions (bad for a large fact table).
    â†’ Best option: **ROUND\_ROBIN**.

* **Partitioning**:

  * Since we delete **by year**, the partition key should be `OrderDateKey`.
  * This allows us to drop old partitions instead of deleting row by row.

---

### Final Answer:

```sql
WITH CLUSTERED COLUMNSTORE INDEX
DISTRIBUTION = ROUND_ROBIN
PARTITION (OrderDateKey RANGE RIGHT FOR VALUES 
(20170101, 20180101, 20190101, 20200101, 20210101))
```

ğŸ‘‰ Correct selections:

* **ROUND\_ROBIN**
* **OrderDateKey**

Letâ€™s break it down:

* The issue: **tempDB contention** identified by **Intelligent Insights** in an **Azure SQL Database**.
* Goal: reduce **contention on tempDB**.

---

### Options analysis:

**A. Implement memory-optimized tables** âœ…

* Memory-optimized tables reduce or eliminate tempDB usage (since no need for tempDB for row versioning or intermediate storage).
* This directly helps reduce tempDB contention.
* Supported in Azure SQL Database.

**B. Run `DBCC FLUSHPROCINDB`** âŒ

* Clears cached execution plans for a database.
* Doesnâ€™t address tempDB contention at all.

**C. Replace sequential index keys with nonsequential keys** âŒ

* This helps reduce latch contention on **indexes** (hotspot issue with identity keys), not **tempDB contention**.

**D. Run `DBCC DBREINDEX`** âŒ

* Deprecated command to rebuild indexes (replaced by `ALTER INDEX`).
* Has nothing to do with tempDB contention.

---

ğŸ‘‰ Correct Answer:
**A. Implement memory-optimized tables**


Letâ€™s analyze the scenario:

* **Requirement:** Count **new events** in **five-minute intervals**.
* **Important detail:** Must **report only events that arrive during the interval** (no late arrivals, no retractions).
* **Sink:** Delta Lake table.

---

### Output modes in Structured Streaming:

* **Complete** â†’ rewrites the entire result table every trigger. Used for aggregations when you want the whole table. Not efficient here.
* **Append** â†’ only new rows (finalized results) are written to the sink. âœ…
* **Update** â†’ only rows that changed since the last trigger are written (good for aggregations with late data).

---

Since the requirement says:

* â€œ**report only events that arrive during the interval**â€ â†’ meaning finalized counts per window, not updates.
* That matches **Append mode**, because results are appended once the window closes.

---

ğŸ‘‰ Correct Answer:
**B. append**

Letâ€™s carefully check the requirements:

* Databases are currently on **SQL Server 2019 (SQL1)**.
* Must be able to **communicate with SQL2 (on-premises)** using **linked server connections**.
* Must **minimize administrative effort**.

---

### Options:

**A. Azure SQL Database** âŒ

* PaaS service, but it does **not support linked servers**.

**B. Azure SQL Database elastic pool** âŒ

* Same limitations as single Azure SQL DB. No linked servers.

**C. SQL Server on Azure Virtual Machines** âœ… (supports linked servers, since itâ€™s full SQL Server).

* But â†’ requires **high administrative effort** (patching, backups, HA, etc.).

**D. Azure SQL Managed Instance** âœ…

* PaaS service with **near 100% SQL Server compatibility**.
* **Supports linked servers** (to on-premises SQL, via private endpoint / VPN / ExpressRoute).
* **Minimizes administrative effort** compared to running SQL on VMs.

---

ğŸ‘‰ Correct Answer:
**D. Azure SQL Managed Instance**


We are designing a **star schema** for analytics.
The extract includes **dimensions (descriptive attributes)** and **facts (measurable values)**.

---

### Given columns:

* **EventCategory** â†’ e.g., "Videos" â†’ **dimension**
* **EventAction** â†’ e.g., "Play" â†’ **dimension**
* **EventLabel** â†’ e.g., "Contoso Promotional" â†’ **dimension**
* **ChannelGrouping** â†’ e.g., "Social" â†’ **dimension**
* **TotalEvents** â†’ numeric â†’ **fact**
* **SessionsWithEvent** â†’ numeric â†’ **fact**
* **UsersWithEvent** â†’ numeric â†’ **fact**

---

### Star schema mapping:

1. **EventCategory** â†’ goes to **DimEvent**
   (Itâ€™s descriptive about the type of event.)

2. **ChannelGrouping** â†’ goes to **DimChannel**
   (It describes the channel, so belongs to channel dimension.)

3. **TotalEvents** â†’ goes to **FactEvents**
   (Itâ€™s a measure/metric, belongs in fact table.)

---

### Final Answer:

* **EventCategory â†’ DimEvent**
* **ChannelGrouping â†’ DimChannel**
* **TotalEvents â†’ FactEvents**


Letâ€™s analyze this step by step:

* **Scenario:**

  * Azure Synapse Analytics (DW1) with **automated data loads**.
  * At the same time, **users run ad-hoc queries**.
  * Requirement: **ensure automated loads have enough memory to finish quickly** despite concurrency.

---

### Options:

**A. Assign a smaller resource class to the automated data load queries** âŒ

* Smaller resource class = less memory â†’ opposite of whatâ€™s needed.

**B. Create sampled statistics to every column** âŒ

* Statistics improve query plans but do not guarantee memory availability for loads.

**C. Assign a larger resource class to the automated data load queries** âœ…

* Resource classes in Synapse determine how much memory is allocated to a query.
* Larger resource class = more memory â†’ better for **data load performance**.

**D. Hash distribute the large fact tables before performing the automated data loads** âŒ

* Hash distribution helps query performance and reduces data movement, but it does **not** solve the memory availability issue for the loading process.

---

ğŸ‘‰ Correct Answer:
**C. Assign a larger resource class to the automated data load queries.**


We need to encrypt **Azure Data Factory v2 (DF1)** using a **customer-managed key (CMK)** from **Azure Key Vault (vault1)**.

---

### Important facts:

* To use **CMK encryption** with ADF, the **Key Vault** must have:

  * **Soft-delete enabled**
  * **Purge protection enabled**

This is a **hard requirement** from Azure because ADF must be able to recover the key if accidentally deleted.

---

### Options:

**A. Disable purge protection on vault1** âŒ

* Wrong, purge protection must be **enabled**, not disabled.

**B. Remove the linked service from df1** âŒ

* Not required before enabling CMK encryption.

**C. Create a self-hosted integration runtime** âŒ

* Not related to encryption with CMK.

**D. Disable soft delete on vault1** âŒ

* Wrong, **soft delete must be enabled**, not disabled.

---

ğŸ‘‰ The correct first step is actually the **opposite** of A and D:
You must **enable soft-delete and purge protection** on the Key Vault.

Since none of the listed answers say **enable**, and the trick options are about disabling them, the correct interpretation is:

âœ… **None of these disabling actions are correct. The required step is to ENABLE soft-delete and purge protection on vault1.**

---




