We need to design the **FactSales** table in **Azure Synapse Analytics dedicated SQL pool** with the following requirements:

* Data will be retained for **five years**.
* Each year, data older than five years must be deleted.
* Data must be **distributed evenly across distributions**.
* Must **minimize deletion time** ‚Üí use **partitioning** by `OrderDateKey`.

---

### Breakdown of choices:

* **Distribution**:

  * `ROUND_ROBIN` ‚Üí spreads rows evenly across distributions (‚úÖ balances load, avoids skew).
  * `HASH` ‚Üí requires a good distribution column (not specified as necessary here).
  * `REPLICATE` ‚Üí duplicates data on all distributions (bad for a large fact table).
    ‚Üí Best option: **ROUND\_ROBIN**.

* **Partitioning**:

  * Since we delete **by year**, the partition key should be `OrderDateKey`.
  * This allows us to drop old partitions instead of deleting row by row.

---

### Final Answer:

```sql
WITH CLUSTERED COLUMNSTORE INDEX
DISTRIBUTION = ROUND_ROBIN
PARTITION (OrderDateKey RANGE RIGHT FOR VALUES 
(20170101, 20180101, 20190101, 20200101, 20210101))
```

üëâ Correct selections:

* **ROUND\_ROBIN**
* **OrderDateKey**

Let‚Äôs break it down:

* The issue: **tempDB contention** identified by **Intelligent Insights** in an **Azure SQL Database**.
* Goal: reduce **contention on tempDB**.

---

### Options analysis:

**A. Implement memory-optimized tables** ‚úÖ

* Memory-optimized tables reduce or eliminate tempDB usage (since no need for tempDB for row versioning or intermediate storage).
* This directly helps reduce tempDB contention.
* Supported in Azure SQL Database.

**B. Run `DBCC FLUSHPROCINDB`** ‚ùå

* Clears cached execution plans for a database.
* Doesn‚Äôt address tempDB contention at all.

**C. Replace sequential index keys with nonsequential keys** ‚ùå

* This helps reduce latch contention on **indexes** (hotspot issue with identity keys), not **tempDB contention**.

**D. Run `DBCC DBREINDEX`** ‚ùå

* Deprecated command to rebuild indexes (replaced by `ALTER INDEX`).
* Has nothing to do with tempDB contention.

---

üëâ Correct Answer:
**A. Implement memory-optimized tables**


Let‚Äôs analyze the scenario:

* **Requirement:** Count **new events** in **five-minute intervals**.
* **Important detail:** Must **report only events that arrive during the interval** (no late arrivals, no retractions).
* **Sink:** Delta Lake table.

---

### Output modes in Structured Streaming:

* **Complete** ‚Üí rewrites the entire result table every trigger. Used for aggregations when you want the whole table. Not efficient here.
* **Append** ‚Üí only new rows (finalized results) are written to the sink. ‚úÖ
* **Update** ‚Üí only rows that changed since the last trigger are written (good for aggregations with late data).

---

Since the requirement says:

* ‚Äú**report only events that arrive during the interval**‚Äù ‚Üí meaning finalized counts per window, not updates.
* That matches **Append mode**, because results are appended once the window closes.

---

üëâ Correct Answer:
**B. append**

Let‚Äôs carefully check the requirements:

* Databases are currently on **SQL Server 2019 (SQL1)**.
* Must be able to **communicate with SQL2 (on-premises)** using **linked server connections**.
* Must **minimize administrative effort**.

---

### Options:

**A. Azure SQL Database** ‚ùå

* PaaS service, but it does **not support linked servers**.

**B. Azure SQL Database elastic pool** ‚ùå

* Same limitations as single Azure SQL DB. No linked servers.

**C. SQL Server on Azure Virtual Machines** ‚úÖ (supports linked servers, since it‚Äôs full SQL Server).

* But ‚Üí requires **high administrative effort** (patching, backups, HA, etc.).

**D. Azure SQL Managed Instance** ‚úÖ

* PaaS service with **near 100% SQL Server compatibility**.
* **Supports linked servers** (to on-premises SQL, via private endpoint / VPN / ExpressRoute).
* **Minimizes administrative effort** compared to running SQL on VMs.

---

üëâ Correct Answer:
**D. Azure SQL Managed Instance**


We are designing a **star schema** for analytics.
The extract includes **dimensions (descriptive attributes)** and **facts (measurable values)**.

---

### Given columns:

* **EventCategory** ‚Üí e.g., "Videos" ‚Üí **dimension**
* **EventAction** ‚Üí e.g., "Play" ‚Üí **dimension**
* **EventLabel** ‚Üí e.g., "Contoso Promotional" ‚Üí **dimension**
* **ChannelGrouping** ‚Üí e.g., "Social" ‚Üí **dimension**
* **TotalEvents** ‚Üí numeric ‚Üí **fact**
* **SessionsWithEvent** ‚Üí numeric ‚Üí **fact**
* **UsersWithEvent** ‚Üí numeric ‚Üí **fact**

---

### Star schema mapping:

1. **EventCategory** ‚Üí goes to **DimEvent**
   (It‚Äôs descriptive about the type of event.)

2. **ChannelGrouping** ‚Üí goes to **DimChannel**
   (It describes the channel, so belongs to channel dimension.)

3. **TotalEvents** ‚Üí goes to **FactEvents**
   (It‚Äôs a measure/metric, belongs in fact table.)

---

### Final Answer:

* **EventCategory ‚Üí DimEvent**
* **ChannelGrouping ‚Üí DimChannel**
* **TotalEvents ‚Üí FactEvents**


Let‚Äôs analyze this step by step:

* **Scenario:**

  * Azure Synapse Analytics (DW1) with **automated data loads**.
  * At the same time, **users run ad-hoc queries**.
  * Requirement: **ensure automated loads have enough memory to finish quickly** despite concurrency.

---

### Options:

**A. Assign a smaller resource class to the automated data load queries** ‚ùå

* Smaller resource class = less memory ‚Üí opposite of what‚Äôs needed.

**B. Create sampled statistics to every column** ‚ùå

* Statistics improve query plans but do not guarantee memory availability for loads.

**C. Assign a larger resource class to the automated data load queries** ‚úÖ

* Resource classes in Synapse determine how much memory is allocated to a query.
* Larger resource class = more memory ‚Üí better for **data load performance**.

**D. Hash distribute the large fact tables before performing the automated data loads** ‚ùå

* Hash distribution helps query performance and reduces data movement, but it does **not** solve the memory availability issue for the loading process.

---

üëâ Correct Answer:
**C. Assign a larger resource class to the automated data load queries.**


We need to encrypt **Azure Data Factory v2 (DF1)** using a **customer-managed key (CMK)** from **Azure Key Vault (vault1)**.

---

### Important facts:

* To use **CMK encryption** with ADF, the **Key Vault** must have:

  * **Soft-delete enabled**
  * **Purge protection enabled**

This is a **hard requirement** from Azure because ADF must be able to recover the key if accidentally deleted.

---

### Options:

**A. Disable purge protection on vault1** ‚ùå

* Wrong, purge protection must be **enabled**, not disabled.

**B. Remove the linked service from df1** ‚ùå

* Not required before enabling CMK encryption.

**C. Create a self-hosted integration runtime** ‚ùå

* Not related to encryption with CMK.

**D. Disable soft delete on vault1** ‚ùå

* Wrong, **soft delete must be enabled**, not disabled.

---

üëâ The correct first step is actually the **opposite** of A and D:
You must **enable soft-delete and purge protection** on the Key Vault.

Since none of the listed answers say **enable**, and the trick options are about disabling them, the correct interpretation is:

‚úÖ **None of these disabling actions are correct. The required step is to ENABLE soft-delete and purge protection on vault1.**

---


We want an **alert on CPU usage** for **all Azure SQL databases** on the same server, including **future databases**.

---

### Options:

**A. Resource Groups** ‚ùå

* Alerts at resource group scope don‚Äôt automatically track database-level metrics like CPU usage.

**B. SQL Servers** ‚úÖ

* If you set the alert on the **SQL Server resource**, it applies to all databases under that server, including **new ones created later**.

**C. SQL Databases** ‚ùå

* Works only per individual database. Would require configuring alerts one by one and wouldn‚Äôt auto-apply to new databases.

**D. SQL Virtual Machines** ‚ùå

* Not relevant here ‚Äî we‚Äôre using **Azure SQL Database PaaS**, not SQL Server on VMs.

---

üëâ Correct Answer:
**B. SQL Servers**

We need **voice notifications** when **maintenance events affect any of the 10 regions** where our **Azure SQL Managed Instances** run.

---

### Options:

**A. From the Azure portal, create a service health alert** ‚úÖ

* Service health alerts notify you about **issues in Azure services or regions** (including planned maintenance).
* Can deliver alerts via **email, SMS, or voice call**.
* Configured **once per subscription/region**, so this minimizes administrative effort across all managed instances.

**B. Create an Azure Advisor operational excellence alert** ‚ùå

* Advisor gives best-practice recommendations (cost, performance, security), not service health/maintenance events.

**C. Configure a SQL Server Agent job in SSMS** ‚ùå

* Local to a single managed instance, doesn‚Äôt integrate with Azure-wide maintenance event notifications.

**D. Configure an activity log alert** ‚ùå

* Activity log captures events **within your subscription resources** (e.g., resource modifications), not **service health events at the regional level**.

---

üëâ Correct Answer:
**A. From the Azure portal, create a service health alert**


Let‚Äôs analyze this step by step:

### Requirements:

1. **Failovers that do not require client applications to change their connection strings**
   ‚Üí This rules out **geo-replication**, because geo-replication requires **manual connection string changes** during failover.

2. **Replicate the database to a secondary Azure region**
   ‚Üí This means cross-region support is needed (not just Availability Zones, which are within the same region).

3. **Support failover to the secondary region**
   ‚Üí Must support **automatic or manual failover across regions**.

---

### Options:

* **A. Failover groups** ‚úÖ

  * Provides **automatic failover** between **primary and secondary servers** across regions.
  * Applications can connect using a **listener endpoint**, so no connection string changes are required.
  * Meets all requirements.

* **B. Transactional replication** ‚ùå

  * Mainly for replication, not high availability or transparent failover.
  * Requires connection changes and doesn‚Äôt provide auto-failover.

* **C. Availability Zones** ‚ùå

  * Increases availability within the **same region**, not across regions.
  * Does not meet the replication-to-secondary-region requirement.

* **D. Geo-replication** ‚ùå

  * Provides readable secondary, but **failover requires manual reconfiguration** of connection strings.
  * Doesn‚Äôt meet the requirement.

---

‚úÖ **Correct Answer: A. failover groups**


Great question üëç Let‚Äôs break it down.

You want to see **the parameter values from the last time a query was executed** on **SQL Server running in an Azure VM**.

---

### üîé Key SQL Server features involved:

1. **Lightweight Query Profiling**

   * Introduced to reduce overhead of query profiling.
   * Enables capturing runtime execution stats (e.g., row counts, operator performance).
   * Needed for getting **execution stats** in DMVs.

2. **Last\_Query\_Plan\_Stats** (Database Scoped Configuration)

   * Allows you to access the **last actual execution plan** for a query (not just the estimated plan).
   * Includes **runtime parameter values** from the last execution.

3. **PARAMETER\_SNIFFING**

   * Controls whether SQL Server caches query plans based on the first set of parameters.
   * Not related to retrieving parameter values from the last query.

---

### ‚úÖ Correct Answers:

* **B. Enable Lightweight\_Query\_Profiling in DB1**
  (Needed to collect query profiling info with minimal overhead.)

* **C. Enable Last\_Query\_Plan\_Stats in DB1**
  (Lets you retrieve the last actual execution plan, including parameter values.)

---

### ‚ùå Incorrect options:

* **A & D (Enable in master)** ‚Üí Wrong scope. These are **database scoped configurations**, not set at master.
* **E (Enable PARAMETER\_SNIFFING)** ‚Üí Controls plan caching, not parameter value retrieval.

---

‚úÖ Final Answer:
**B. Enable Lightweight\_Query\_Profiling in DB1**
**C. Enable Last\_Query\_Plan\_Stats in DB1**

---


Alright, let‚Äôs carefully analyze this one ‚úÖ

---

### Requirements:

1. **Support user-initiated backups**

   * Needed for flexibility (restore when you want, not only automated retention).
   * ‚úÖ Available in **SQL Server on Azure VM** (IaaS) ‚Üí you control SQL Server Agent jobs, backups to Blob Storage, etc.
   * ‚ùå Azure SQL Database / Managed Instance ‚Üí backups are automated by Azure (you can restore but not *initiate* backups directly).

2. **Support multiple automatically replicated instances across Azure regions**

   * Means geo-redundancy / high availability.

3. **Minimize administrative effort**

   * Rules out heavy management like Always On AG setup in SQL VMs.

---

### Deployment Option:

üëâ **SQL Server on Azure Virtual Machines**

* Because only in IaaS SQL VM can you run **user-initiated backups**.
* PaaS (Azure SQL Database / Managed Instance) = automatic backups only, no user-initiated.

---

### Resiliency Option:

üëâ **Auto-failover group**

* Provides **automatic replication across regions** + automatic failover with **no connection string change**.
* Active geo-replication = database-level only (manual failover).
* Zone-redundant deployment = only within the same region, not across regions.

---

### ‚úÖ Correct Answer:

* **Deployment option**: **SQL Server on Azure Virtual Machines**
* **Resiliency option**: **Auto-failover group**


Great, let‚Äôs break this one down carefully üëá

---

### **NEW QUESTION 49 (Exam Topic 5)**

You have an Azure SQL Database.
You are reviewing a **slow performing query**. The screenshot shows a query plan with operators.

---

### **Step 1 ‚Äì Identify what the exhibit shows**

* In the query plan screenshot, the header indicates "Query cost relative to the batch: 100%".
* The boxes show query execution operators with percentages.
* This is a **visual execution plan** that SQL Server produces when you run queries.
* Since it shows **estimated vs actual execution details** (with percentages), this is an **actual execution plan**.

üëâ **Answer:** **an actual execution plan**

---

### **Step 2 ‚Äì Identify the operator in the plan**

Looking at the exhibit:

* The bottom operator is labeled **Clustered Index Scan**.
* But the question asks for the **operator in the execution plan that indicates the query would benefit from tuning**.
* The operator causing performance issues here is **Key Lookup (Clustered)** (since key lookups repeatedly access rows from a clustered index ‚Äî expensive for performance).

üëâ **Answer:** **Key Lookup**

---

### ‚úÖ Final Answer:

1. The exhibit shows ‚Üí **an actual execution plan**
2. The operator in the execution plan that indicates query would benefit from performance tuning ‚Üí **Key Lookup**

---

‚ö° Tip: In SQL exams, whenever you see **Key Lookup** or **Index Scan** with high cost %, it usually means missing indexes ‚Üí performance tuning required.

Good question üëá

We need anomaly detection for **streaming IoT hub data** with **minimum development effort**. Let‚Äôs evaluate:

---

### **Requirements**

* **Send output to Azure Synapse** ‚Üí must support Synapse sink integration.
* **Identify spikes and dips in time series** ‚Üí anomaly detection on real-time streaming data.
* **Minimize development/configuration effort** ‚Üí prefer serverless, built-in features over heavy coding.

---

### **Option Analysis**

**A. Azure SQL Database**

* Not suitable for anomaly detection on streaming data.
* It‚Äôs more for storage, not real-time analytics.
  ‚ùå

**B. Azure Databricks**

* Powerful for machine learning and advanced analytics.
* But requires **custom ML models**, coding, and more configuration.
* Does not minimize effort.
  ‚ùå

**C. Azure Stream Analytics**

* Has **built-in anomaly detection functions** (e.g., `AnomalyDetection_SpikeAndDip`).
* Natively integrates with **Azure IoT Hub (input)** and **Azure Synapse Analytics (output sink)**.
* **Serverless, minimal setup**, exactly fits requirements.
  ‚úÖ

---

### ‚úÖ Correct Answer:

**C. Azure Stream Analytics**

---

Great one üëá This is about ensuring your **Azure Stream Analytics job has enough Streaming Units (SUs)** provisioned.

---

### **Key Metric: SU % Utilization**

* This shows how much of your allocated Streaming Unit capacity is being used.
* If it's close to 100%, you may need to scale up.

But monitoring SU utilization alone is not enough. You need to check if the job is keeping up with the input data stream.

---

### **Options Analysis**

**A. Late Input Events**

* These occur when events arrive later than the configured *late arrival tolerance*.
* They‚Äôre about data quality, not SU capacity.
  ‚ùå

**B. Out of order Events**

* These are events that arrive out of sequence.
* Also about event ordering, not about whether SUs are sufficient.
  ‚ùå

**C. Backlogged Input Events**

* Very important.
* Shows how many input events the job has received but not yet processed.
* If backlog grows ‚Üí SUs are insufficient.
  ‚úÖ

**D. Watermark Delay**

* Indicates how far the system is behind in processing events, measured in time.
* If watermark delay is increasing ‚Üí job is under-provisioned.
  ‚úÖ

**E. Function Events**

* Refers to events sent to Azure Functions (custom processing).
* Not relevant to SU sizing.
  ‚ùå

---

### ‚úÖ Correct Answers:

**C. Backlogged Input Events**
**D. Watermark Delay**

---


### **Question Recap**

* DB: **Azure SQL database (db1)**
* Monitoring tool: **Query Performance Insight (QPI)** ‚Üí relies on **Query Store**.
* Requirement: **Performance monitoring data must be available as soon as possible**.
* Task: Choose **configuration setting** and **value**.

---

### **Understanding Query Store Configurations**

1. **DATA\_FLUSH\_INTERVAL\_SECONDS**

   * Controls how often collected data is written (‚Äúflushed‚Äù) from memory to disk.
   * **Default = 900s (15 min)**.
   * Lowering this value (e.g., 60s or 1s) makes data available faster in QPI.

2. **INTERVAL\_LENGTH\_MINUTES**

   * Defines time granularity of data aggregation (default = 60 min).
   * Smaller intervals (e.g., 1 min) improve granularity but don‚Äôt directly affect how soon data is available.

3. **MAX\_PLANS\_PER\_QUERY**

   * Controls how many execution plans are stored ‚Üí irrelevant for timeliness.

4. **QUERY\_CAPTURE\_MODE**

   * Controls which queries are captured (ALL, AUTO, CUSTOM).
   * Impacts **what is captured**, not **when it is available**.

---

### **Correct Answer**

To make **performance monitoring data available ASAP**, you must:

* Configure **DATA\_FLUSH\_INTERVAL\_SECONDS**
* Set to **1 (second)**

---

‚úÖ **Final Answer:**
**Configuration setting:** `DATA_FLUSH_INTERVAL_SECONDS`
**Value:** `1`

---

### Command structure:

```sql
RESTORE DATABASE MyDB1
FROM DISK = 'D:\DB1.bak'
WITH { option }
GO
```

---

### Options:

1. **NORECOVERY**

   * Leaves the database **non-operational**.
   * Used when you plan to apply **additional transaction log backups**.
   * Keeps the database in a "restoring" state.
   * Example:

     ```sql
     RESTORE DATABASE MyDB1 
     FROM DISK = 'D:\DB1.bak' 
     WITH NORECOVERY;
     ```

2. **RECOVERY**

   * Makes the database **operational** immediately after the restore.
   * You cannot restore more backups after this.
   * Example:

     ```sql
     RESTORE DATABASE MyDB1 
     FROM DISK = 'D:\DB1.bak' 
     WITH RECOVERY;
     ```

3. **STANDBY**

   * Makes the database **read-only** after restore.
   * Useful for log shipping (secondary database is readable while waiting for the next log restore).
   * Example:

     ```sql
     RESTORE DATABASE MyDB1 
     FROM DISK = 'D:\DB1.bak' 
     WITH STANDBY = 'D:\UNDO\DB1.undo';
     ```

---

‚úÖ **In short:**

* Use **NORECOVERY** if more restores are coming (like log backups).
* Use **RECOVERY** if this is the final restore and you want the DB online.
* Use **STANDBY** if you want a read-only copy (common in log shipping).


### Key point in the question:

* Customers need to be able to **create database objects**.
* The solution must meet **business goals** ‚Üí usually means **isolation** between customers, avoiding conflicts.

---

### Roles:

* **`ddl_admin`**: Allows a user to run **Data Definition Language (DDL)** commands such as `CREATE`, `ALTER`, `DROP`.
* **`db_writer`**: Only allows `INSERT`, `UPDATE`, `DELETE` data ‚Üí **cannot create schema objects**.

---

### Options:

**A. Grant `ddl_admin` on existing schema**

* Problem: All customers share the same schema ‚Üí conflicts, no isolation. ‚ùå

**B. Create additional schema per customer and grant `ddl_admin`**

* ‚úÖ Best option.
* Each customer gets their own schema, and `ddl_admin` on that schema allows them to create their own tables, procedures, etc.
* Prevents conflicts, aligns with multi-tenant isolation.

**C. Create additional schema and grant `db_writer`**

* ‚ùå `db_writer` only allows DML, not schema changes.

**D. Grant `db_writer` on existing schema**

* ‚ùå Same as above, plus shared schema (bad for isolation).

---

‚úÖ **Correct Answer: B. For each customer, create an additional schema and grant the customer ddl\_admin to the new schema.**

---

### Scenario:

* You need to implement **security** for **ResearchDB1**.
* Requirement ‚Üí must meet **security and compliance**.
* Clearly, this is about **Always Encrypted** with **Azure Key Vault** integration.

---

### Steps (in correct sequence):

1. **Create an Azure AD managed identity**

   * Needed so ResearchApp1 can authenticate securely without storing credentials.

2. **Create an Azure Key Vault instance and configure an access policy**

   * Key Vault stores encryption keys.
   * Access policy ensures the managed identity has permission to use the keys.

3. **Run the Always Encrypted wizard**

   * This wizard configures column encryption and points to the Key Vault for key management.

---

### Answer:

‚úÖ Correct sequence is:

1. **Create an Azure AD managed identity**
2. **Create an Azure Key Vault instance and configure an access policy**
3. **Run the Always Encrypted wizard**

---

‚ö°Pro tip:
The option **‚ÄúCreate an Azure Key Vault instance and generate a secret‚Äù** is a distractor. For Always Encrypted, you store **keys**, not secrets.

---


Good question üëå Let‚Äôs analyze it.

---

### Scenario:

* You have **an Azure SQL Database server** (`sqlsrv1`) with 10 databases.
* Performance is slower than expected.
* You specifically want to check **if the issue is related to `tempdb`**.

---

### Options:

**A. Run Query Store-based queries**

* Query Store helps with **query performance** over time (execution plans, regressions).
* Not about `tempdb`. ‚ùå

**B. Review information provided by SQL Server Profiler-based traces**

* SQL Profiler is not supported for Azure SQL Database. ‚ùå

**C. Review information provided by Query Performance Insight**

* Gives insights into **top consuming queries** (DTU, CPU, IO, memory).
* Doesn‚Äôt specifically help diagnose `tempdb` usage. ‚ùå

**D. Run dynamic management view (DMV)-based queries**

* DMVs in Azure SQL Database can show resource usage, waits, and specifically **tempdb contention** (e.g., `sys.dm_db_resource_stats`, `sys.dm_exec_requests`, `sys.dm_db_session_space_usage`).
* This is the correct way to check `tempdb` bottlenecks. ‚úÖ

---

### ‚úÖ Correct Answer:

**D. Run dynamic management view-based queries**

---

Great question üëç Let‚Äôs break it down clearly:

---

### **Why ‚ÄúSimple layout (striping)‚Äù is the correct choice here**

* You have **two P30 disks**, each provides **5,000 IOPS**.
* Requirement: **10,000 IOPS**.
* The **simple layout** (striping) combines both disks‚Äô throughput and IOPS, giving you **aggregate performance** (5,000 + 5,000 = 10,000 IOPS).
* This meets the requirement.

---

### **Other layouts and when they are used**

1. **Mirror layout**

   * Writes are duplicated across disks.
   * You don‚Äôt get combined IOPS; instead, you get **redundancy**.
   * Example: If you had two disks with 5,000 IOPS each, effective IOPS remains 5,000.
   * Use case: **High availability and redundancy** (at the cost of performance/space).

2. **Parity layout**

   * Data + parity bits are striped across disks.
   * Provides fault tolerance with better capacity than mirror, but **slower writes**.
   * Use case: **Archival or workloads that need balance between capacity and resiliency**.
   * (Not shown in your options, but good to know.)

3. **Simple layout (striping)**

   * Data is striped across disks, **no redundancy**.
   * All disks contribute to performance (IOPS + throughput).
   * Use case: **High-performance workloads where redundancy is not critical** (e.g., tempdb, non-critical analytical workloads, or when redundancy is provided elsewhere).

4. **Storage Pool**

   * Logical grouping of multiple physical disks.
   * Foundation step before creating virtual disks with layouts.
   * Use case: Always the **first step** when using Storage Spaces in Windows/Azure VM.

---

‚úÖ In your scenario:

* Performance (10,000 IOPS) is the **primary requirement**.
* Redundancy is **not mentioned**.
* So ‚Üí **Storage Pool ‚Üí Simple Layout ‚Üí Volume** is correct.

---


Correct ‚úÖ

The answer is: **B. Decrease by half the value of Data Flush Interval**

### Explanation:

* Query Store collects query performance data in memory first, then periodically flushes it to disk (the database).
* If the **Data Flush Interval** is too long, Query Store can accumulate too much data in memory. If SQL Server runs out of space for Query Store, it can transition into **read-only mode** to prevent further growth.
* By **decreasing the Data Flush Interval**, data is written to disk more frequently, reducing memory pressure and minimizing the risk of Query Store becoming read-only.

Other options:

* **A. Double Data Flush Interval** ‚Üí Increases risk (keeps more data in memory).
* **C & D. Statistics Collection Interval** ‚Üí Controls how often runtime statistics are aggregated, not related to Query Store read-only transitions.

üëâ So the best solution is **B**.

Perfect ‚úÖ let‚Äôs break this down step by step using the policy in the screenshot.

### **Retention Policy in Screenshot**

* **Weekly LTR Backups** ‚Üí Retained for **6 weeks**.
* **Monthly LTR Backups** ‚Üí Retained for **12 months** (the **Week 2 backup** of each month).
* **Yearly LTR Backups** ‚Üí Retained for **10 years** (the **Week 2 backup of January** each year).

---

### **Example: January 2020**

1. **Jan 4, 2020** ‚Üí Week 1 of Jan

   * Stored as **weekly backup**.
   * Retained **6 weeks** only.
   * ‚ùå Not monthly, because policy says Week 2.

2. **Jan 11, 2020** ‚Üí Week 2 of Jan

   * Stored as **weekly backup** ‚Üí retained **6 weeks**.
   * ‚úÖ Promoted to **monthly backup** ‚Üí retained **12 months**.
   * ‚úÖ Also promoted to **yearly backup** (since it‚Äôs the Week 2 backup of January) ‚Üí retained **10 years**.

---

### **February 2020**

* **Feb 1, 2020** ‚Üí Week 1 ‚Üí weekly only ‚Üí 6 weeks.
* **Feb 8, 2020** ‚Üí Week 2 ‚Üí weekly + promoted to monthly ‚Üí retained **12 months**.

(no yearly in February, only January provides yearly).

---

### **Summary**

* **Weekly backups**: Every week, kept **6 weeks**.
* **Monthly backups**: The **Week 2 backup** of each month, kept **12 months**.
* **Yearly backups**: The **Week 2 backup of January**, kept **10 years**.

---

üëâ So the reason **Jan 4, 2020** is **not** a monthly backup is simply because the **monthly rule is configured for Week 2**, not Week 1.
